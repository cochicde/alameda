hourlyPredict = false

[watchdog]
delayedSec = 120
  [watchdog.model]
  directory = "/tmp/model"
  [watchdog.predict]
  directory = "/tmp/predict"

[datahub]
address = "datahub.alameda.svc.cluster.local:50050"
connRetry = 5
connTimeout = 30 # seconds
  [datahub.query]
  retry = 3
  retryInterval = 10 # seconds

[queue]
url = "amqp://admin:adminpass@rabbitmq.alameda.svc.cluster.local:5672"
  [queue.retry]
  publishTime = 5
  publishIntervalMs = 3000
  consumeTime = 5
  consumeIntervalMs = 3000
  connectIntervalMs = 3000
  ackTimeoutSec = 3
  [queue.consumer]
  reconnectInterval = 30 #seconds

[serviceSetting]
granularities = ["30s", "1m", "1h", "6h", "24h"]
predictUnits = ["POD", "GPU", "NAMESPACE",
  "APPLICATION", "CLUSTER", "CONTROLLER", "NODE"
]
# must put NODE predict unit at last, because to send
# NODE jobs with granularity 30s depends on POD job
# with granularity 30s are sent

[granularities]

  [granularities.24h]
  dataGranularity = "24h"
  dataGranularitySec = 86400
  predictionSteps = 30
  predictionJobSendIntervalSec = 86400
  modelJobSendIntervalSec = 86400
  modelMaxUsedTimes = 11

  [granularities.6h]
  dataGranularity = "6h"
  dataGranularitySec = 21600
  predictionSteps = 30
  predictionJobSendIntervalSec = 21600
  modelJobSendIntervalSec = 21600
  modelMaxUsedTimes = 11

  [granularities.1h]
  dataGranularity = "1h"
  dataGranularitySec = 3600
  predictionSteps = 30
  predictionJobSendIntervalSec = 3600
  modelJobSendIntervalSec = 3600
  modelMaxUsedTimes = 11

  [granularities.1m]
  dataGranularity = "1m"
  dataGranularitySec = 60
  predictionSteps = 60
  predictionJobSendIntervalSec = 60
  modelJobSendIntervalSec = 60
  modelMaxUsedTimes = 29

  [granularities.30s]
  dataGranularity = "30s"
  dataGranularitySec = 30
  predictionSteps = 30
  predictionJobSendIntervalSec = 30
  modelJobSendIntervalSec = 30
  modelMaxUsedTimes = 29

[predictUnits]

  [predictUnits.POD]
  type = "POD"

  [predictUnits.NODE]
  type = "NODE"

  [predictUnits.GPU]
  type = "GPU"

  [predictUnits.NAMESPACE]
  type = "NAMESPACE"

  [predictUnits.APPLICATION]
  type = "APPLICATION"

  [predictUnits.CLUSTER]
  type = "CLUSTER"

  [predictUnits.CONTROLLER]
  type = "CONTROLLER"

[log]
setLogcallers = true
outputLevel = "info" # debug, info, warn, error, fatal, none

[model]
enabled = false
timeout = 180

[measurements]
  current = "mape"
  minimumDataPoints = 5
  maximumDataPoints = 5
  [measurements.mape]
  threshold = 15
  [measurements.rmse]
  threshold = 10
    [measurements.rmse.normalization]
    cpu = 1 #millicores
    memory = 1000000 #bytes
    dutyCycle = 0.2

# api proto metric type
# alameda_api/v1alpha1/datahub/common/metrics.proto
[metricType]
undefined = 0
cpu_seconds_total = 1
cpu_cores_allocatable = 2
cpu_millicores_total = 3
cpu_millicores_avail = 4
cpu_millicores_usage = 5
cpu_millicores_usage_pct = 6
memory_bytes_allocatable = 7
memory_bytes_total = 8
memory_bytes_avail = 9
memory_bytes_usage = 10
memory_bytes_usage_pct = 11
fs_bytes_total = 12
fs_bytes_avail = 13
fs_bytes_usage = 14
fs_bytes_usage_pct = 15
http_requests_count = 16
http_requests_total = 17
http_response_count = 18
http_response_total = 19
disk_io_seconds_total = 20
disk_io_utilization = 21
restarts_total = 22
unschedulable = 23
health = 24
power_usage_watts = 25
temperature_celsius = 26
duty_cycle = 27
current_offset = 28
lag = 29
latency = 30
number = 31

# api proto table
# alameda_api/v1alpha1/datahub/schemas/types.proto
[scope]
undefined = 0
application = 1
fedemeter = 2
metric = 3
planning = 4
prediction = 5
recommendation = 6
resource = 7
target = 8

# api proto aggregation function
# common/common.proto
[aggregation]
none = 0
max = 1
avg = 2

[[units]]
enabled = true
scope = "application"
category = "kafka"
type = "topic"
measurement = "kafka_topic"
idKeys = ["cluster_name", "namespace", "name"]
granularities = ["1m"]
metricTypes = ["current_offset"]
predictor = "SARIMAX"
[units.valueKeys]
  scalerNamespace = "alameda_scaler_namespace"
  scalerName = "alameda_scaler_name"

  [units.metric]
  scope = "metric"
  category = "kafka"
  type = "topic"
  aggregation = "avg"
    [units.metric.valueKeys]
    value = "value"

  [units.prediction]
  scope = "prediction"
  category = "kafka"
  type = "topic"
    [units.prediction.valueKeys]
    modelID = "model_id"
    predictID = "prediction_id"
    granularity = "granularity"
    value = "value"


[[units]]
enabled = true
scope = "application"
category = "kafka"
type = "consumer_group"
measurement = "kafka_consumer_group"
idKeys = ["cluster_name", "namespace", "name", "topic_name"]
granularities = ["1m"]
metricTypes = ["current_offset"]
predictor = "SARIMAX"
[units.valueKeys]
  scalerNamespace = "alameda_scaler_namespace"
  scalerName = "alameda_scaler_name"
  resourceK8SNamespace = "resource_k8s_namespace"
  resourceK8SName = "resource_k8s_name"

  [units.metric]
  scope = "metric"
  category = "kafka"
  type = "consumer_group"
  aggregation = "avg"
    [units.metric.valueKeys]
    value = "value"

  [units.prediction]
  scope = "prediction"
  category = "kafka"
  type = "consumer_group"
    [units.prediction.valueKeys]
    modelID = "model_id"
    predictID = "prediction_id"
    granularity = "granularity"
    value = "value"